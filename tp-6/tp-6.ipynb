{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lmatayoshi/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/lmatayoshi/Documents/Projects/repo-ciencia-datos/tp-6/ap/\"\n",
    "filename = base_dir + \"ap.txt\"\n",
    "filename2 = base_dir + \"vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as f:\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_no = []\n",
    "texts = []\n",
    "for i in range(len(lines)):\n",
    "    if i % 6 == 1:\n",
    "        doc_no.append(lines[i].split(\" \")[1])\n",
    "    if i % 6 == 3:\n",
    "        tokens = nltk.word_tokenize(lines[i])\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        texts.append( tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dict = dict(zip(doc_no, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename2, \"r\") as f:\n",
    "    vocab = []\n",
    "    for word in f:\n",
    "        vocab.append(word.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_dicc=[]\n",
    "for i in range(len(texts)):\n",
    "    general_dicc.append(Counter(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = [0] * len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuency_dict = dict(zip(vocab, zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    for word in general_dicc[i].keys():\n",
    "        if word in frecuency_dict.keys():\n",
    "            frecuency_dict[word] += general_dicc[i][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_frecuency = sorted(frecuency_dict.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_selected = dict(sorted_frecuency[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(inf)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.misc.factorial(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.65277777777778"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500 * 499 / 2 / 60.0 / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4527"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(texts[0], window_size= 10)\n",
    "len(finder.score_ngrams(bigram_measures.pmi))\n",
    "#for i in finder.score_ngrams(bigram_measures.pmi):\n",
    "#    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BigramAssocMeasures in module nltk.metrics.association:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |      n_ii counts (w1, w2), i.e. the bigram being scored\n",
      " |      n_ix counts (w1, *)\n",
      " |      n_xi counts (*, w2)\n",
      " |      n_xx counts (*, *), i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(cls, n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset([])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(cls, *marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.collocations.BigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"this is a foo bar bar black sheep  foo bar bar black sheep foo bar bar black sheep shep bar bar black sentence\"\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(word_tokenize(text))\n",
    "\n",
    "for i in finder.score_ngrams(trigram_measures.pmi):\n",
    "    print i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
